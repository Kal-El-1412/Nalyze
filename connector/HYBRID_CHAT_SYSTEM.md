# Hybrid Chat System: Intent-Based + Free-Text

## Overview

The chat system supports two interaction modes that coexist seamlessly:

1. **Free-Text Mode** - Exploratory, flexible, LLM-powered
2. **Intent Mode** - Deterministic, fast, state-based

## Visual Comparison

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      USER INTERFACE                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                             ‚îÇ                                   ‚îÇ
‚îÇ   üìù TEXT INPUT BOX         ‚îÇ   üîò CLARIFICATION BUTTONS        ‚îÇ
‚îÇ   "Show me top customers"   ‚îÇ   [Trend] [Summary] [Comparison]  ‚îÇ
‚îÇ                             ‚îÇ                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                              ‚îÇ
               ‚îÇ                              ‚îÇ
               ‚ñº                              ‚ñº
        FREE-TEXT MODE                  INTENT MODE
               ‚îÇ                              ‚îÇ
               ‚ñº                              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  POST /chat          ‚îÇ      ‚îÇ  POST /chat          ‚îÇ
    ‚îÇ  {                   ‚îÇ      ‚îÇ  {                   ‚îÇ
    ‚îÇ    message: "..."    ‚îÇ      ‚îÇ    intent: "...",    ‚îÇ
    ‚îÇ  }                   ‚îÇ      ‚îÇ    value: "..."      ‚îÇ
    ‚îÇ                      ‚îÇ      ‚îÇ  }                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                              ‚îÇ
               ‚ñº                              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  LLM PROCESSING      ‚îÇ      ‚îÇ  STATE UPDATE        ‚îÇ
    ‚îÇ  - Interprets text   ‚îÇ      ‚îÇ  - Direct update     ‚îÇ
    ‚îÇ  - Full NLU          ‚îÇ      ‚îÇ  - No LLM call       ‚îÇ
    ‚îÇ  - Flexible          ‚îÇ      ‚îÇ  - Instant           ‚îÇ
    ‚îÇ  - ~2-3 sec          ‚îÇ      ‚îÇ  - ~50ms             ‚îÇ
    ‚îÇ  - $0.001-0.01       ‚îÇ      ‚îÇ  - Free              ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                              ‚îÇ
               ‚ñº                              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  queries OR           ‚îÇ      ‚îÇ  intent_acknowledged ‚îÇ
    ‚îÇ  needs_clarification  ‚îÇ      ‚îÇ                      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                              ‚ñº
                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                   ‚îÇ  Frontend sends:     ‚îÇ
                                   ‚îÇ  { message: "cont." }‚îÇ
                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                              ‚ñº
                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                   ‚îÇ  Backend checks      ‚îÇ
                                   ‚îÇ  state ‚Üí next step   ‚îÇ
                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Request Flow Comparison

### Free-Text Flow

```
User types: "What are my top customers by revenue?"

Frontend:
  handleSendMessage("What are my top customers by revenue?")

Request:
  POST /chat
  {
    "datasetId": "sales-2024",
    "conversationId": "conv-123",
    "message": "What are my top customers by revenue?"
  }

Backend:
  1. Load state (context from previous intents)
  2. Call LLM with message + context
  3. LLM processes natural language
  4. LLM generates SQL queries OR asks clarification

Response:
  {
    "type": "run_queries",
    "queries": [
      {
        "name": "top_customers",
        "sql": "SELECT customer_name, SUM(revenue) as total FROM sales GROUP BY customer_name ORDER BY total DESC LIMIT 10"
      }
    ]
  }

Result: Queries executed, results returned
```

**Characteristics:**
- ‚úÖ Flexible natural language
- ‚úÖ Exploratory analysis
- ‚úÖ LLM understands intent
- ‚è±Ô∏è 2-3 seconds
- üí∞ $0.001-0.01 per request

---

### Intent Flow

```
User clicks: [Trend] button

Frontend:
  handleClarificationResponse("Trend", "set_analysis_type")

Request:
  POST /chat
  {
    "datasetId": "sales-2024",
    "conversationId": "conv-123",
    "intent": "set_analysis_type",
    "value": "Trend"
  }

Backend:
  1. Update state: { analysis_type: "Trend" }
  2. Return acknowledgment (NO LLM call)

Response:
  {
    "type": "intent_acknowledged",
    "intent": "set_analysis_type",
    "value": "Trend",
    "state": {
      "conversation_id": "conv-123",
      "context": {
        "analysis_type": "Trend"
      }
    }
  }

Frontend:
  POST /chat
  {
    "datasetId": "sales-2024",
    "conversationId": "conv-123",
    "message": "continue"
  }

Backend:
  1. Check state: { analysis_type: "Trend" }
  2. Still missing time_period
  3. Return next clarification

Response:
  {
    "type": "needs_clarification",
    "question": "What time period?",
    "choices": ["Last 7 days", "Last 30 days", "Last 90 days"]
  }

Result: Next clarification shown
```

**Characteristics:**
- ‚úÖ Deterministic state update
- ‚úÖ No LLM interpretation
- ‚úÖ Instant response
- ‚è±Ô∏è ~50ms
- üí∞ Free

---

## Mixed Mode Example

### Complete User Journey

```
1. User types: "Show me trends"
   ‚Üí POST /chat { message: "Show me trends" }
   ‚Üí LLM processes, identifies need for clarification
   ‚Üí Response: needs_clarification (analysis_type)

2. User clicks: [Trend]
   ‚Üí POST /chat { intent: "set_analysis_type", value: "Trend" }
   ‚Üí State updated (no LLM)
   ‚Üí Response: intent_acknowledged
   ‚Üí Frontend: POST /chat { message: "continue" }
   ‚Üí Response: needs_clarification (time_period)

3. User clicks: [Last 30 days]
   ‚Üí POST /chat { intent: "set_time_period", value: "Last 30 days" }
   ‚Üí State updated (no LLM)
   ‚Üí Response: intent_acknowledged
   ‚Üí Frontend: POST /chat { message: "continue" }
   ‚Üí Backend: All fields present, call LLM
   ‚Üí Response: run_queries

4. Queries executed, results shown

5. User types: "Can you also show year-over-year?"
   ‚Üí POST /chat { message: "Can you also show year-over-year?" }
   ‚Üí LLM processes with existing state context
   ‚Üí Response: run_queries (additional analysis)
```

**Key Points:**
- Started with free-text
- Used intents for clarifications (fast)
- Returned to free-text for follow-up (flexible)
- State preserved throughout
- Best of both worlds

---

## Routing Decision Tree

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Interaction      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                ‚îÇ
    ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Types  ‚îÇ      ‚îÇ Clicks ‚îÇ
‚îÇ Text   ‚îÇ      ‚îÇ Button ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ               ‚îÇ
     ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ          ‚îÇ         ‚îÇ
     ‚îÇ          ‚ñº         ‚ñº
     ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ    ‚îÇ Intent  ‚îÇ ‚îÇ   No    ‚îÇ
     ‚îÇ    ‚îÇ Detected‚îÇ ‚îÇ Intent  ‚îÇ
     ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ         ‚îÇ           ‚îÇ
     ‚ñº         ‚ñº           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   handleSendMessage()      ‚îÇ
‚îÇ   ‚Üí { message: "..." }     ‚îÇ
‚îÇ   ‚Üí LLM processes          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   handleClarificationResp  ‚îÇ
‚îÇ   ‚Üí { intent: "...", ... } ‚îÇ
‚îÇ   ‚Üí State updated          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Code Structure

### Text Input Handler

```typescript
// src/pages/AppLayout.tsx

const handleSendMessage = async (content: string) => {
  // Create user message for display
  const userMessage: Message = {
    id: Date.now().toString(),
    type: 'user',
    content,
    timestamp: new Date().toLocaleTimeString(),
  };
  setMessages([...messages, userMessage]);

  // Send to backend with message field
  const result = await connectorApi.sendChatMessage({
    datasetId: activeDataset,
    conversationId,
    message: content,  // ‚Üê Free-text message
    defaultsContext: defaults,
  });

  // Process response
  await handleChatResponse(result.data);
};
```

---

### Button Click Handler

```typescript
// src/pages/AppLayout.tsx

const handleClarificationResponse = async (
  choice: string,
  intent?: string
) => {
  // Create user message for display
  const userMessage: Message = {
    id: Date.now().toString(),
    type: 'user',
    content: choice,
    timestamp: new Date().toLocaleTimeString(),
  };
  setMessages(prev => [...prev, userMessage]);

  if (intent) {
    // Send intent request
    const result = await connectorApi.sendChatMessage({
      datasetId: activeDataset,
      conversationId,
      intent,        // ‚Üê Intent name
      value: choice, // ‚Üê Intent value
    });

    await handleChatResponse(result.data);

    // Explicit follow-up
    const followUp = await connectorApi.sendChatMessage({
      datasetId: activeDataset,
      conversationId,
      message: 'continue',
    });

    await handleChatResponse(followUp.data);
  } else {
    // No intent, fall back to text
    handleSendMessage(choice);
  }
};
```

---

## Backend Handling

```python
# app/chat_orchestrator.py

@router.post("/chat")
async def chat(request: ChatOrchestratorRequest):
    # Validation
    if request.message and request.intent:
        raise HTTPException(400, "Cannot provide both")
    if not request.message and not request.intent:
        raise HTTPException(400, "Must provide either")

    # Intent-based (deterministic)
    if request.intent:
        # Update state directly
        state_manager.update_state(
            conversation_id=request.conversationId,
            context={
                request.intent.replace("set_", ""): request.value
            }
        )

        # Return acknowledgment (no LLM)
        return IntentAcknowledgmentResponse(
            type="intent_acknowledged",
            intent=request.intent,
            value=request.value,
            state=state_manager.get_state(request.conversationId)
        )

    # Message-based (LLM processing)
    if request.message:
        # Load state
        context = state_manager.get_state(request.conversationId)

        # Check if clarifications needed
        missing = check_required_fields(context)
        if missing:
            return NeedsClarificationResponse(
                type="needs_clarification",
                question=get_clarification_question(missing[0]),
                choices=get_choices(missing[0])
            )

        # All fields present, call LLM
        llm_response = await llm.process(
            message=request.message,
            context=context,
            catalog=catalog
        )

        return llm_response
```

---

## Performance Comparison

| Metric | Free-Text Mode | Intent Mode |
|--------|----------------|-------------|
| **Latency** | 2-3 seconds | ~50ms |
| **Cost** | $0.001-0.01 | Free |
| **LLM Calls** | 1 per message | 0 |
| **Determinism** | Variable | 100% |
| **Flexibility** | High | Fixed options |
| **Use Case** | Exploration | Clarifications |

---

## Benefits

### For Users

‚úÖ **Type when exploring** - Natural language questions
‚úÖ **Click when clarifying** - Fast, clear options
‚úÖ **Switch freely** - No mode switching needed
‚úÖ **No repeated questions** - State remembered
‚úÖ **Instant feedback** - Button clicks instant

### For Developers

‚úÖ **Separation of concerns** - Backend handles clarifications, LLM handles analysis
‚úÖ **Cost optimization** - Intents bypass LLM
‚úÖ **Predictable behavior** - Intents deterministic
‚úÖ **Flexible extension** - Add new intents easily
‚úÖ **Backward compatible** - Old code works

### For Product

‚úÖ **Better UX** - Right tool for each interaction
‚úÖ **Lower cost** - Fewer LLM calls
‚úÖ **Faster response** - Instant state updates
‚úÖ **Reliable** - Deterministic clarifications
‚úÖ **Scalable** - State-based, not LLM-based

---

## Fallback Behavior

If a clarification question doesn't match known intent patterns:

```typescript
// Frontend detects no intent
const intent = detectIntentFromQuestion(
  "Which specific column should I use?"
);
// Returns: undefined

// Falls back to text-based
if (intent) {
  // Send intent
} else {
  // Send as regular message
  handleSendMessage(choice);
}
```

**Result:** ‚úÖ Graceful degradation, backward compatible

---

## Summary

The hybrid system provides:

**Free-Text Mode:**
- User types questions
- LLM interprets natural language
- Flexible, exploratory
- ~2-3 seconds response time

**Intent Mode:**
- User clicks buttons
- State updated directly
- Deterministic, fast
- ~50ms response time

**Both Together:**
- Seamless coexistence
- No mode switching
- State shared across both
- Best tool for each interaction

**Result:** Fast, cheap, flexible chat experience with perfect separation between exploration (LLM) and clarification (state).
